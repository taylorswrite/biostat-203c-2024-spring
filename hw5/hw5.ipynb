{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c0b83a-e325-4c30-85af-389850b4678b",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "Due date: Jun 2, 2024\n",
    "\n",
    "### Submission instructions: \n",
    "- __Autograder will be used for scoring, you are required to write a module in a file `hw5module.py` as in Homeworks 1 and 2.__ \n",
    "- You are also required to convert this notebook as a separate Python file, __`hw5.py`__. \n",
    "- Also, please keep your indentifiable information (ID, name, and a list of collaborators) in a separate file __`hw5_studentinfo.txt`__. \n",
    "- Submit __`hw5.ipynb`__ (this notebook), `hw5.py`, `hw5module.py`, and `hw5_studentinfo.txt` on Gradescope under the window \"Homework 5 - code\". Do **NOT** change the file name. This will be checked by the autograder as well. \n",
    "- Make sure all your code and text outputs in the problems are visible in your PDF submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d5fde-8e9d-4a2a-ad79-7f60d6ea3316",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "What's your favorite movie? Wouldn't it be nice to find more shows that you might like to watch, based on ones you know you like? Tools that address questions like this are often called \"recommender systems.\" Powerful, scalable recommender systems are behind many modern entertainment and streaming services, such as Netflix and Spotify. While most recommender systems these days involve machine learning, there are also ways to make recommendations that don't require such complex tools. \n",
    "\n",
    "In this homework, you'll use webscraping to answer the following question: \n",
    "\n",
    "> What movie or TV shows share actors with your favorite movie?\n",
    "\n",
    "The idea of this question is that, if the movie Y has many of the same actors as the movie X, and you like X, you might also enjoy Y. \n",
    "\n",
    "This homework has two parts. In the first, larger part, you'll write a webscraper for finding shared actors on TMDB. In the second, smaller part, you'll use the results from your scraper to make recommendations. \n",
    "\n",
    "You need to meet the specifications for a complete list of what you need to do to obtain full credit. \n",
    "\n",
    "## Instructions\n",
    "\n",
    "### 1. Setup\n",
    "\n",
    "#### 1.1. Locate the Starting TMDB Page\n",
    "\n",
    "Pick your favorite movie, and locate its TMDB page by searching on [https://www.themoviedb.org/](https://www.themoviedb.org/). For example, my favorite movie is _Harry Potter and the ~~Sorcerer's~~Philosopher's Stone_.\n",
    "Its TMDB page is at:  \n",
    "\n",
    "\n",
    "https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\n",
    "\n",
    "\n",
    "Save this URL for a moment. \n",
    "\n",
    "#### 1.2. Dry-Run Navigation\n",
    "\n",
    "Now, we're just going to practice clicking through the navigation steps that our scraper will take. \n",
    "\n",
    "First, click on the *Full Cast & Crew* link. This will take you to a page with URL of the form \n",
    "\n",
    "```\n",
    "<original_url>cast/\n",
    "```\n",
    "\n",
    "Next, scroll until you see the *Cast* section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL. For example, the URL for Alan Rickman, who played Severus Snape, is \n",
    "\n",
    "\n",
    "https://www.themoviedb.org/person/4566-alan-rickman\n",
    "\n",
    "\n",
    "Finally, scroll down until you see the actor's *Acting* section. Note the titles of a few movies and TV shows in this section. \n",
    "\n",
    "Our scraper is going to replicate this process. Starting with your favorite movie, it's going to look at all the actors in that movie, and then log all the *other* movies or TV shows that they worked on. \n",
    "\n",
    "At this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n",
    "\n",
    "#### 1.3. Create your module\n",
    "\n",
    "No template is provided for this homework. You will write your two functions in a separate file `hw5module.py`.\n",
    "\n",
    "#### 1.4. Some hints\n",
    "\n",
    "You may run into `403` (forbidden) errors once the website detects that you’re a bot. See the web scraping lecture note and these links ([link1](https://doc.scrapy.org/en/latest/topics/practices.html#avoiding-getting-banned), [link2](https://scrapeops.io/python-scrapy-playbook/scrapy-403-unhandled-forbidden-error/), [link3](https://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/), [link4](https://scrapingrobot.com/blog/most-common-user-agents/)) for how to work around that issue. Adding a delay for each page and changing user agent will often be most helpful!\n",
    "\n",
    "Keep an eye out for `403` error you see! Make sure to examine the `status_code` attribute of the returned value from `requests.get()`. You want your status to be `200` (meaning OK). Print something if you see `403` (or raise an `Exception` if you are familiar with it).\n",
    "If they know that you are on Python or if you are requesting pages without much delays, they will certainly try to block you. One way to change user agent on your code is presented in the lecture note. For the autograder to finish in reasonable time, please do not put the delays longer than two seconds between requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d5439-0213-4b86-9d9f-ea7562840ee2",
   "metadata": {},
   "source": [
    "### 2. Write Your Scraper\n",
    "\n",
    "Now, you will write a web scraper for a movie of your choice by giving its subdirectory on TMDB website as an argument. We will implement two parsing functions.\n",
    "\n",
    "<!-- `parse(suffix)` should assume that you start on a movie page, and then navigate to the *Full Cast & Crew* page. Remember that this page has url `<movie_url>cast`. (You are allowed to hardcode that part.) Once there, the `parse_full_credits(suffix)` should be called. The `parse()` function does not return any data. This function should be no more than 5 lines of code, excluding comments and docstrings. Example: `parse(\"671-harry-potter-and-the-philosopher-s-stone\")`.-->\n",
    "- `parse_full_credits(movie_directory)` should assume that you start on the *Full Cast & Crew* page with the url `https://www.themoviedb.org/movie/<movie_directory>/cast`. Its purpose is to call the function `parse_actor_page(df, actor_directory)` for the page of each actor listed on the page. Crew members are not included (consider using `not` command in CSS selector). Initialize an empty `DataFrame` with two columns `actor` and `movie_or_TV_name`, then call the function `parse_actor_page` for each actor. The `parse_full_credits()` function returns the fully loaded `df`, with actor names and movie titles each actor worked on. The `DataFrame` should not have duplicate entries, and it should be sorted by actor name as the primary key, then movie titles. Try to avoid visiting the same page multiple times. This function should be no more than 10 lines of code, excluding comments and docstrings. \n",
    "  - Example: `df = parse_full_credits(\"671-harry-potter-and-the-philosopher-s-stone\")`\n",
    "- `parse_actor_page(df, actor_directory)` should assume that you start on the page of an actor. For each movie with the \"Acting\" role, you will add a row to the `DataFrame` `df` with two columns, `actor` and `movie_or_TV_name`. Please only include the works listed in \"Acting\" section of the actor page. Keep in mind that \"Acting\" might not be on the top of their lists; for example, [David Holmes](https://www.themoviedb.org/person/1796507-david-holmes) is credited with an acting role in HP1, but spent most of his career as a stunt double of Daniel Radcliffe (as a part of Crew). On his page, you will see \"Crew\" before \"Acting\". \n",
    "Note that you will need to determine both the name of the actor and the name of each movie or TV show through parsing the HTML page. This function should be no more than 20 lines of code, excluding comments and docstrings. It should return the `DataFrame` `df` with all the works of the actor added at the end of `df`. \n",
    "  - Example: `df_updated = parse_actor_page(df, \"10980-daniel-radcliffe\")`\n",
    "\n",
    "Provided that these functions are correctly implemented, you can run the code \n",
    "```python\n",
    "df = parse_full_credits(\"671-harry-potter-and-the-philosopher-s-stone\")\n",
    "```\n",
    "to create a `DataFrame` with a column for actors and another for movies or TV shows for _Harry Potter and the Philosopher's Stone_. You might want to save the result as a `.csv` file before proceeding to the next part.\n",
    "\n",
    "Test your functions; make sure to check the following: \n",
    "- `parse_actor_page()`\n",
    "  - only parses all the works under the \"Acting\" section\n",
    "  - even if \"Acting\" is not on the top of the lists\n",
    "  - remove duplicate work names within each actor (added 5/23)\n",
    "- `parse_full_credits()` \n",
    "  - is parsing all the actors,\n",
    "  - is not parsing crew members,\n",
    "  - does not parse duplicate pages, and\n",
    "  - of course, if the results are correct.\n",
    "\n",
    "#### Challenge\n",
    "If you’re looking for a challenge, think about ways that may make your recommendations more accurate. Consider scraping the number of episodes as well or limiting the number of actors you get per show to make sure you only get the main series cast. If you do so, please use separate function names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf783c-cc5a-4a5f-bfcb-feb0be824888",
   "metadata": {},
   "source": [
    "Function parse_full_credits(movie_directory):\n",
    "- Start on the Full Cast & Crew page of a movie, accessed via URL https://www.themoviedb.org/movie/<movie_directory>/cast.\n",
    "- Initialize an empty DataFrame with columns actor and movie_or_TV_name.\n",
    "- For each actor listed on the page (excluding crew members, which may require conditional logic with CSS selectors like :not):\n",
    "- Call the function parse_actor_page(df, actor_directory) to parse each actor’s page.\n",
    "- Return the DataFrame populated with actor names and the movie titles they've worked on.\n",
    "- Ensure no duplicate entries; sort by actor name and then by movie titles.\n",
    "- This function should be concise (aim for no more than 10 lines of code, excluding comments and docstrings).\n",
    "\n",
    "Function parse_actor_page(df, actor_directory):\n",
    "- Start on an individual actor’s page accessed via https://www.themoviedb.org/person/<actor_directory>.\n",
    "- Only parse works listed under the \"Acting\" section, regardless of its position on the page.\n",
    "- Add each movie or TV show the actor has worked on to the DataFrame under the columns actor and movie_or_TV_name.\n",
    "- Ensure no duplicate entries within each actor's list of works.\n",
    "- This function should also be concise (aim for no more than 20 lines of code, excluding comments and docstrings).\n",
    "- Return the updated DataFrame df.\n",
    "\n",
    "Example Usage:\n",
    "You can test the scraper by running df = parse_full_credits(\"671-harry-potter-and-the-philosopher-s-stone\") to fetch cast data for Harry Potter and the Philosopher's Stone.\n",
    "Optionally, save the results to a .csv file for further analysis or backup.\n",
    "Testing Requirements:\n",
    "\n",
    "Ensure parse_actor_page() only processes entries under the \"Acting\" section and handles pages where \"Acting\" is not the first category.\n",
    "Ensure parse_full_credits() parses all actors (not crew members), avoids duplicate pages, and returns correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ab17e9-e3f5-41cc-8671-1520fff6b75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U beautifulsoup4 requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5d4a555-220e-4eba-a6d8-c9fc13739ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_actor_page(actor_movies, actor_directory):\n",
    "    response = requests.get(actor_directory)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    ## in progres.\n",
    "    \n",
    "    \n",
    "def parse_full_credits(movie_directory):\n",
    "    response = requests.get(movie_directory)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    cast_crud = soup.select(\".pad:nth-child(1) a\")\n",
    "    cast_name = [link.text for link in cast_crud if \" \" in link.text]\n",
    "    cast_name_link = [name.lower().replace(\" \", \"-\") for name in cast_name]\n",
    "    cast_link = np.unique(\n",
    "        [f\"https://www.themoviedb.org{link['href']}?credit_department=Acting\" \n",
    "         for link in cast_crud]\n",
    "    )\n",
    "    cast_dict =  dict(zip(cast_name, cast_name_link))\n",
    "    \n",
    "    actor_movies = pl.DataFrame(\n",
    "    dict(actor=[], movie_or_TV_name=[]),\n",
    "        schema=dict(actor=pl.String, movie_or_TV_name=pl.String),\n",
    "    )\n",
    "    for actor, actor_directory in cast_dict.items():\n",
    "        parse_actor_page(actor_movies, actor_directory)\n",
    "\n",
    "\n",
    "url = \"https://www.themoviedb.org/movie/786892-furiosa-a-mad-max-saga/cast\"\n",
    "parse_full_credits(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b1f93b9-26fc-4cea-ba52-ed48f4878ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Movies from Cast\n",
    "url = \"https://www.themoviedb.org/person/74568-chris-hemsworth?credit_department=Acting\"\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = BeautifulSoup(data, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61084f33-ee2b-481a-867d-df053962dee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project X\n",
      "Thor 5\n",
      "Crime 101\n",
      "Extraction 3\n",
      "Untitled Hulk Hogan Biopic\n",
      "Down Under Cover\n",
      "Transformers One\n",
      "Furiosa: A Mad Max Saga\n",
      "Arnold & Sly: Rivals, Friends, Icons\n",
      "Extraction 2\n",
      "Kath & Kim: Our Effluent Life\n",
      "Limitless with Chris Hemsworth\n",
      "Marvel Studios Assembled: The Making of Thor: Love and Thunder\n",
      "Thor: Love and Thunder\n",
      "Spiderhead\n",
      "Interceptor\n",
      "Marvel Studios' 2021 Disney+ Day Special\n",
      "What If...?\n",
      "Shark Beach With Chris Hemsworth\n",
      "Loki\n",
      "Marvel Studios Assembled\n",
      "Marvel Studios Legends\n",
      "Extraction\n",
      "Celebrating Marvel's Stan Lee\n",
      "Jay and Silent Bob Reboot\n",
      "The Kelly Clarkson Show\n",
      "Men in Black: International\n",
      "Avengers: Endgame\n",
      "Bad Times at the El Royale\n",
      "Beyond Boundaries: The Harvey Weinstein Scandal\n",
      "Avengers: Infinity War\n",
      "Team Darryl\n",
      "12 Strong\n",
      "Thor: Ragnarok\n",
      "Team Thor: Part 2\n",
      "Doctor Strange\n",
      "Team Thor\n",
      "Ghostbusters\n",
      "The Huntsman: Winter's War\n",
      "Phase 2 Tag Scenes: A Making of the Marvel Cinematic Universe Phase Two\n",
      "In the Heart of the Sea\n",
      "The Late Show with Stephen Colbert\n",
      "Vacation\n",
      "Avengers: Age of Ultron\n",
      "Hot Ones\n",
      "Blackhat\n",
      "Planeta Calleja\n",
      "Marvel Studios: Assembling a Universe\n",
      "A Brothers' Journey: Thor & Loki\n",
      "The Tonight Show Starring Jimmy Fallon\n",
      "Thor: The Dark World\n",
      "Rush\n",
      "Heroes and Demons\n",
      "Snow White and the Huntsman\n",
      "The Avengers\n",
      "Evening Urgant\n",
      "The Cabin in the Woods\n",
      "Red Dawn\n",
      "Honest Trailers\n",
      "Thor: Assembling the Troupe\n",
      "Thor: From Asgard to Earth\n",
      "Thor\n",
      "Ollie Klublershturf vs. the Nazis\n",
      "Ca$h\n",
      "A Perfect Getaway\n",
      "Star Trek\n",
      "The Graham Norton Show\n",
      "Dancing with the Stars\n",
      "Fergus McPhail\n",
      "Marshall Law\n",
      "Guinevere Jones\n",
      "The Saddle Club\n",
      "LIVE with Kelly and Mark\n",
      "Home and Away\n",
      "Neighbours\n",
      "Entertainment Tonight\n",
      "Saturday Night Live\n",
      "The Oscars\n"
     ]
    }
   ],
   "source": [
    "# Extract elements using CSS selector\n",
    "elements = soup.select('.tooltip bdi')\n",
    "# Print the text of each element\n",
    "for element in elements:\n",
    "    print(element.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8fb8c39-e053-4406-8d30-6641ff0340de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.DataFrame(\n",
    "    dict(actor=[], movie_or_TV_name=[]),\n",
    "    schema=dict(actor=pl.String, movie_or_TV_name=pl.String),\n",
    ")\n",
    "for key, value in cast_dict.items():\n",
    "    requests.get(value)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a5069d-c776-43ff-afbc-b64f3262fd65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.themoviedb.org/movie/786892-furiosa-a-mad-max-saga/cast\"\n",
    "\n",
    "def parse_actor_page(df, actor_directory):\n",
    "    \n",
    "def parse_full_credits(movie_directory):\n",
    "    response = requests.get(movie_directory)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    pl.DataFrame(dict(actor=[], movie_or_TV_name=[]))\n",
    "    for actor in cast:\n",
    "        parse_actor_page(df, actor_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16111bc7-bcea-4711-824b-6e5493bf55a1",
   "metadata": {},
   "source": [
    "### 3. Make Your Recommendations \n",
    "\n",
    "Once you're happy with the operation of your webscraper, compute a sorted list with the top movies and TV shows that share actors with your favorite movie. For example, it may have two columns: one for “movie names” and “number of shared actors”.\n",
    "\n",
    "Feel free to be creative. You can show a pandas data frame, a chart using `matplotlib` or `plotly`, or any other sensible display of the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516382a5-336b-4301-bbd1-ad90f8414f44",
   "metadata": {},
   "source": [
    "### 4. Documentation\n",
    "\n",
    "In this Jupyter Notebook, you should describe how your scraper works, as well as the results of your analysis. When describing your scraper, I recommend dividing it up into the two distinct parsing function, and discussing them one-by-one. For example: \n",
    "\n",
    "> *In this report, I'm going to make a super cool web scraper...*\n",
    "> *Here's how we set up the project...*\n",
    "> ```\n",
    "> <implementation of parse()>\n",
    "> ```\n",
    "> *This function works by...*\n",
    "\n",
    "> ```\n",
    "> <implementation of parse_full_credits()>\n",
    "> ```\n",
    "> *To write this function, I...*\n",
    "\n",
    "In addition to describing your scraper, your report should include a table and visualization of numbers of shared actors. \n",
    "\n",
    "You should guide your reader through the process of setting up and running the scraper.\n",
    "\n",
    "\n",
    "## Specifications\n",
    "\n",
    "### Coding Problem\n",
    "\n",
    "1. Each of the three parsing methods are correctly implemented.\n",
    "2. `parse()` is implemented in no more than 5 lines. \n",
    "3. `parse_full_credits()` is implemented in no more than 5 lines. \n",
    "4. `parse_actor_page()` is implemented in no more than 15 lines. \n",
    "5. A table or list of results or pandas dataframe is shown.\n",
    "6. A visualization with `matplotlib`, `plotly`, or `seaborn` is shown. \n",
    "\n",
    "### Style and Documentation\n",
    "\n",
    "7. Each of the three `parse` functions has a short docstring describing its assumptions (e.g. what kind of page it is meant to parse) and its effect, including navigation and data outputs. \n",
    "8. Each of the three `parse` functions has helpful comments for understanding how each chunk of code operates. \n",
    "\n",
    "### Writing\n",
    "\n",
    "9. The report is written in engaging and clear English. Grammar and spelling errors are acceptable within reason. \n",
    "10. The report explains clearly how to set up the project, run the scraper, and access the results. \n",
    "11. The report explains how each of the three `parse` methods works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c1ccd-cacb-409c-8bae-33e06a3aa0da",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://stackoverflow.com/questions/48938108/how-to-get-css-links-using-beautiful-soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "203C",
   "language": "python",
   "name": "203c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
